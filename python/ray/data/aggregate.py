import math
from typing import TYPE_CHECKING, Any, Callable, List, Optional, Union


from ray.data._internal.planner.exchange.sort_task_spec import SortKey
from ray.data._internal.util import is_nan
from ray.data.block import AggType, Block, BlockAccessor, KeyType, T, U
from ray.util.annotations import PublicAPI, Deprecated

if TYPE_CHECKING:
    import pyarrow as pa


@Deprecated(message="AggregateFn is deprecated, please use AggregateFnV2")
@PublicAPI
class AggregateFn:
    """NOTE: THIS IS DEPRECATED, PLEASE USE AggregateFnV2 INSTEAD

    Defines how to perform a custom aggregation in Ray Data.

    `AggregateFn` instances are passed to a Dataset's ``.aggregate(...)`` method to
    specify the steps required to transform and combine rows sharing the same key.
    This enables implementing custom aggregators beyond the standard
    built-in options like Sum, Min, Max, Mean, etc.

    Args:
        init: Function that creates an initial aggregator for each group. Receives a key
            (the group key) and returns the initial accumulator state (commonly 0,
            an empty list, or an empty dictionary).
        merge: Function that merges two accumulators generated by different workers
            into one accumulator.
        name: An optional display name for the aggregator. Useful for debugging.
        accumulate_row: Function that processes an individual row. It receives the current
            accumulator and a row, then returns an updated accumulator. Cannot be
            used if `accumulate_block` is provided.
        accumulate_block: Function that processes an entire block of rows at once. It receives the
            current accumulator and a block of rows, then returns an updated accumulator.
            This allows for vectorized operations. Cannot be used if `accumulate_row`
            is provided.
        finalize: Function that finishes the aggregation by transforming the final
            accumulator state into the desired output. For example, if your
            accumulator is a list of items, you may want to compute a statistic
            from the list. If not provided, the final accumulator state is returned
            as-is.

    Example:
        .. testcode::

            import ray
            from ray.data.aggregate import AggregateFn

            # A simple aggregator that counts how many rows there are per group
            count_agg = AggregateFn(
                init=lambda k: 0,
                accumulate_row=lambda counter, row: counter + 1,
                merge=lambda c1, c2: c1 + c2,
                name="custom_count"
            )
            ds = ray.data.from_items([{"group": "A"}, {"group": "B"}, {"group": "A"}])
            result = ds.groupby("group").aggregate(count_agg).take_all()
            # result: [{'group': 'A', 'custom_count': 2}, {'group': 'B', 'custom_count': 1}]
    """

    def __init__(
        self,
        init: Callable[[KeyType], AggType],
        merge: Callable[[AggType, AggType], AggType],
        name: str,
        _merge_batch: Callable[[List[AggType]], AggType] = None,
        accumulate_row: Callable[[AggType, T], AggType] = None,
        accumulate_block: Callable[[AggType, Block], AggType] = None,
        finalize: Optional[Callable[[AggType], U]] = None,
    ):
        if (accumulate_row is None and accumulate_block is None) or (
            accumulate_row is not None and accumulate_block is not None
        ):
            raise ValueError(
                "Exactly one of accumulate_row or accumulate_block must be provided."
            )

        if accumulate_block is None:

            def accumulate_block(a: AggType, block: Block) -> AggType:
                block_acc = BlockAccessor.for_block(block)
                for r in block_acc.iter_rows(public_row_format=False):
                    a = accumulate_row(a, r)
                return a

        if _merge_batch is None:

            def _merge_batch(partially_agg_vals: List[AggType]) -> AggType:
                if len(partially_agg_vals) == 0:
                    return None

                cur_acc = partially_agg_vals[0]
                for v in partially_agg_vals[1:]:
                    cur_acc = merge(cur_acc, v)

                return cur_acc

        if not isinstance(name, str):
            raise TypeError("`name` must be provided.")

        if finalize is None:
            finalize = lambda a: a  # noqa: E731

        self.name = name
        self.init = init
        self._merge_batch = _merge_batch
        self.accumulate_block = accumulate_block
        self.finalize = finalize

    def _validate(self, schema: Optional[Union[type, "pa.lib.Schema"]]) -> None:
        """Raise an error if this cannot be applied to the given schema."""
        pass


@PublicAPI(stability="beta")
class AggregateFnV2(AggregateFn):
    """Provides an interface to implement efficient aggregations to be applied
    to the dataset.

    `AggregateFnV2` instances are passed to a Dataset's ``.aggregate(...)`` method to
    perform aggregations by applying distributed aggregation algorithm:

        - `aggregate_block` is applied to individual blocks, producing partial
            aggregations.
        - `merge` combines multiple partially aggregated values (previously returned
            from `aggregate_block` partial aggregations into a singular partial
            aggregation.
        - `finalize` transforms partial aggregation into its final state (for
            some aggregations this is an identity transformation, ie no-op)

    """

    def __init__(
        self,
        name: str,
        *,
        ignore_nulls: bool,
        merge: Callable[[AggType, AggType], AggType],
        aggregate_block: Callable[[Block], Optional[AggType]],
        finalize: Optional[Callable[[AggType], U]] = None,
    ):
        assert name, f"Non-empty string has to be provided as name (got {name})"

        if finalize is None:
            _safe_finalize = lambda a: a  # noqa: E731
        else:

            def _safe_finalize(acc):
                if _is_null(acc):
                    return None

                return finalize(acc)

        _safe_merge = _null_safe_merge(merge, ignore_nulls)

        super().__init__(
            name=name,
            init=lambda _: None,
            merge=_safe_merge,
            accumulate_block=lambda acc, block: _safe_merge(
                acc, aggregate_block(block)
            ),
            finalize=_safe_finalize,
        )


class _AggregateOnKeyBase(AggregateFnV2):
    def _set_key_fn(self, on: str):
        self._key_fn = on

    def _validate(self, schema: Optional[Union[type, "pa.lib.Schema"]]) -> None:
        SortKey(self._key_fn).validate_schema(schema)


@PublicAPI
class Count(_AggregateOnKeyBase):
    """Defines count aggregation."""

    def __init__(
        self,
        on: Optional[str] = None,
        ignore_nulls: bool = False,
        alias_name: Optional[str] = None,
    ):
        self._set_key_fn(on)

        if alias_name:
            self._rs_name = alias_name
        else:
            self._rs_name = f"count({str(on)})"

        super().__init__(
            self._rs_name,
            ignore_nulls=ignore_nulls,
            aggregate_block=(lambda block: BlockAccessor.for_block(block).num_rows()),
            merge=lambda a1, a2: a1 + a2,
        )


@PublicAPI
class Sum(_AggregateOnKeyBase):
    """Defines sum aggregation."""

    def __init__(
        self,
        on: Optional[str] = None,
        ignore_nulls: bool = True,
        alias_name: Optional[str] = None,
    ):
        self._set_key_fn(on)
        if alias_name:
            self._rs_name = alias_name
        else:
            self._rs_name = f"sum({str(on)})"

        super().__init__(
            self._rs_name,
            ignore_nulls=ignore_nulls,
            merge=lambda a1, a2: a1 + a2,
            aggregate_block=(
                lambda block: BlockAccessor.for_block(block).sum(on, ignore_nulls)
            ),
        )


@PublicAPI
class Min(_AggregateOnKeyBase):
    """Defines min aggregation."""

    def __init__(
        self,
        on: Optional[str] = None,
        ignore_nulls: bool = True,
        alias_name: Optional[str] = None,
    ):
        self._set_key_fn(on)
        if alias_name:
            self._rs_name = alias_name
        else:
            self._rs_name = f"min({str(on)})"

        super().__init__(
            self._rs_name,
            ignore_nulls=ignore_nulls,
            merge=min,
            aggregate_block=(
                lambda block: BlockAccessor.for_block(block).min(on, ignore_nulls)
            ),
        )


@PublicAPI
class Max(_AggregateOnKeyBase):
    """Defines max aggregation."""

    def __init__(
        self,
        on: Optional[str] = None,
        ignore_nulls: bool = True,
        alias_name: Optional[str] = None,
    ):
        self._set_key_fn(on)
        if alias_name:
            self._rs_name = alias_name
        else:
            self._rs_name = f"max({str(on)})"

        super().__init__(
            self._rs_name,
            ignore_nulls=ignore_nulls,
            merge=max,
            aggregate_block=lambda block: BlockAccessor.for_block(block).max(
                on, ignore_nulls
            ),
        )


@PublicAPI
class Mean(_AggregateOnKeyBase):
    """Defines mean aggregation."""

    def __init__(
        self,
        on: Optional[str] = None,
        ignore_nulls: bool = True,
        alias_name: Optional[str] = None,
    ):
        self._set_key_fn(on)
        if alias_name:
            self._rs_name = alias_name
        else:
            self._rs_name = f"mean({str(on)})"

        def vectorized_mean(block: Block) -> AggType:
            block_acc = BlockAccessor.for_block(block)
            count = block_acc.count(on)
            if count == 0 or count is None:
                # Empty or all null.
                return None
            sum_ = block_acc.sum(on, ignore_nulls)
            if sum_ is None:
                # ignore_nulls=False and at least one null.
                return None
            return [sum_, count]

        super().__init__(
            self._rs_name,
            ignore_nulls=ignore_nulls,
            merge=lambda a1, a2: [a1[0] + a2[0], a1[1] + a2[1]],
            aggregate_block=vectorized_mean,
            finalize=lambda a: a[0] / a[1],
        )


@PublicAPI
class Std(_AggregateOnKeyBase):
    """Defines standard deviation aggregation.

    Uses Welford's online method for an accumulator-style computation of the
    standard deviation. This method was chosen due to its numerical
    stability, and it being computable in a single pass.
    This may give different (but more accurate) results than NumPy, Pandas,
    and sklearn, which use a less numerically stable two-pass algorithm.
    See
    https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Welford's_online_algorithm
    """

    def __init__(
        self,
        on: Optional[str] = None,
        ddof: int = 1,
        ignore_nulls: bool = True,
        alias_name: Optional[str] = None,
    ):
        self._set_key_fn(on)
        if alias_name:
            self._rs_name = alias_name
        else:
            self._rs_name = f"std({str(on)})"

        def merge(a: List[float], b: List[float]):
            # Merges two accumulations into one.
            # See
            # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Parallel_algorithm
            M2_a, mean_a, count_a = a
            M2_b, mean_b, count_b = b
            delta = mean_b - mean_a
            count = count_a + count_b
            # NOTE: We use this mean calculation since it's more numerically
            # stable than mean_a + delta * count_b / count, which actually
            # deviates from Pandas in the ~15th decimal place and causes our
            # exact comparison tests to fail.
            mean = (mean_a * count_a + mean_b * count_b) / count
            # Update the sum of squared differences.
            M2 = M2_a + M2_b + (delta**2) * count_a * count_b / count
            return [M2, mean, count]

        def vectorized_std(block: Block) -> AggType:
            block_acc = BlockAccessor.for_block(block)
            count = block_acc.count(on)
            if count == 0 or count is None:
                # Empty or all null.
                return None
            sum_ = block_acc.sum(on, ignore_nulls)
            if sum_ is None:
                # ignore_nulls=False and at least one null.
                return None
            mean = sum_ / count
            M2 = block_acc.sum_of_squared_diffs_from_mean(on, ignore_nulls, mean)
            return [M2, mean, count]

        def finalize(a: List[float]):
            # Compute the final standard deviation from the accumulated
            # sum of squared differences from current mean and the count.
            M2, mean, count = a
            if count < 2:
                return 0.0
            return math.sqrt(M2 / (count - ddof))

        super().__init__(
            self._rs_name,
            ignore_nulls=ignore_nulls,
            merge=merge,
            aggregate_block=vectorized_std,
            finalize=finalize,
        )


@PublicAPI
class AbsMax(_AggregateOnKeyBase):
    """Defines absolute max aggregation."""

    def __init__(
        self,
        on: Optional[str] = None,
        ignore_nulls: bool = True,
        alias_name: Optional[str] = None,
    ):
        self._set_key_fn(on)
        if alias_name:
            self._rs_name = alias_name
        else:
            self._rs_name = f"abs_max({str(on)})"

        if on is None or not isinstance(on, str):
            raise ValueError(f"Column to aggregate on has to be provided (got {on})")

        def _null_safe_abs(v):
            return abs(v) if not _is_null(v) else None

        def _aggregate(block: Block):
            block_accessor = BlockAccessor.for_block(block)

            max_ = block_accessor.max(on, ignore_nulls)
            min_ = block_accessor.min(on, ignore_nulls)

            return max(_null_safe_abs(max_), _null_safe_abs(min_))

        super().__init__(
            self._rs_name,
            ignore_nulls=ignore_nulls,
            merge=max,
            aggregate_block=_aggregate,
        )


@PublicAPI
class Quantile(_AggregateOnKeyBase):
    """Defines Quantile aggregation."""

    def __init__(
        self,
        on: Optional[str] = None,
        q: float = 0.5,
        ignore_nulls: bool = True,
        alias_name: Optional[str] = None,
    ):
        self._set_key_fn(on)
        self._q = q
        if alias_name:
            self._rs_name = alias_name
        else:
            self._rs_name = f"quantile({str(on)})"

        def merge(a: List[int], b: List[int]):
            if isinstance(a, List) and isinstance(b, List):
                a.extend(b)
                return a
            if isinstance(a, List) and (not isinstance(b, List)):
                if b is not None and b != "":
                    a.append(b)
                return a
            if isinstance(b, List) and (not isinstance(a, List)):
                if a is not None and a != "":
                    b.append(a)
                return b

            ls = []
            if a is not None and a != "":
                ls.append(a)
            if b is not None and b != "":
                ls.append(b)
            return ls

        def _list_column(block: Block) -> AggType:
            block_acc = BlockAccessor.for_block(block)
            ls = []
            for row in block_acc.iter_rows(public_row_format=False):
                ls.append(row.get(on))
            return ls

        import math

        def percentile(input_values, key: Optional[Callable[[Any], Any]] = None):
            if not input_values:
                return None

            if key is None:
                key = lambda x: x  # noqa: E731

            input_values = sorted(input_values)
            k = (len(input_values) - 1) * self._q
            f = math.floor(k)
            c = math.ceil(k)
            if f == c:
                return key(input_values[int(k)])
            d0 = key(input_values[int(f)]) * (c - k)
            d1 = key(input_values[int(c)]) * (k - f)
            return round(d0 + d1, 5)

        super().__init__(
            self._rs_name,
            ignore_nulls=ignore_nulls,
            merge=merge,
            aggregate_block=_list_column,
            finalize=percentile,
        )


@PublicAPI
class Unique(_AggregateOnKeyBase):
    """Defines unique aggregation."""

    def __init__(
        self,
        on: Optional[str] = None,
        alias_name: Optional[str] = None,
    ):
        self._set_key_fn(on)
        if alias_name:
            self._rs_name = alias_name
        else:
            self._rs_name = f"unique({str(on)})"

        def to_set(x):
            if isinstance(x, set):
                return x
            elif isinstance(x, list):
                return set(x)
            else:
                return {x}

        def block_row_unique(block: Block) -> AggType:
            import pyarrow.compute as pac

            col = BlockAccessor.for_block(block).to_arrow().column(on)
            return pac.unique(col).to_pylist()

        def merge(a, b):
            return to_set(a) | to_set(b)

        super().__init__(
            name=(self._rs_name),
            ignore_nulls=False,
            merge=merge,
            aggregate_block=block_row_unique,
        )


def _is_null(a: Optional[AggType]) -> bool:
    return a is None or is_nan(a)


def _null_safe_merge(
    merge: Callable[[AggType, AggType], AggType],
    ignore_nulls: bool,
) -> Callable[[Optional[AggType], Optional[AggType]], Optional[AggType]]:
    def _safe_merge(
        cur: Optional[AggType], new: Optional[AggType]
    ) -> Optional[AggType]:
        # Null-safe merge implements following semantic (see in-line)
        if _is_null(cur):
            #   - If the current accumulated value is null (NaN or None), return
            #       newly aggregated value (from new block)
            return new
        elif _is_null(new):
            #   - If the null (NaN or None) has been produced by the aggregation AND
            #       ignore_nulls=False, returned value is prioritized and returned immediately
            #   - If ignore_nulls=True, aggregation could only produce nulls if the block
            #       holds only null values. In that case we consider this case same as block
            #       being empty and return currently accumulated value
            return new if not ignore_nulls else cur
        else:
            #   - Otherwise, values are merged (using provided merging util)
            return merge(cur, new)

    return _safe_merge
